.. _model_name_means:

===================
大模型命名含义
===================

.. note::

   本文根据gemini文档进行整理，以便能够理解业界的规则

在 `Ollama Models <https://ollama.com/search>`_ 中可以看到 `Ollama llama3.3 <https://ollama.com/library/llama3.3/tags>`_ 下各种模型规格的命名，典型的有:

``llama3.3:70b-instruct-q4_K_M``
=================================

- ``instruct`` 表示这个模型是经过 **指令微调 (Instruction Fine-tuned)** 的版本

  - **Base模型(基座模型)** : 只学习了“预测下一个词”。如果你给它发“请写一段 Python 代码”，它可能会接着写“这是很多人常问的问题...”，而不是真的去写代码。它更像是一个超级搜索引擎。
  - **Instruct模型(指令模型)** : 在基座模型基础上，通过大量的"问题-回答"进行强化训练。它明白自己是一个 **助手** ，当你下令时，它会严格执行指令。
  - 结论: 在99% 的对话、编程、办公场景下，你应该 **永远选择** ``instruct`` 版本。

- ``q4_K_M``

  - ``q`` 表示Quantized (量化)
  - ``4`` 表示4-bit(4位)。原始模型通常是16位(FP16)。4-bit意味着将模型参数的精度降低，从而让模型文件体积缩小到原来的1/4左右
  - ``K`` 表示 ``K-quant`` ，这是目前主流的高级量化算法，它不是除暴地压缩，而是通过"分组"优化，让模型在压缩的同时尽量少丢"智力"
  - ``M`` 表示 ``Medium`` (中等)，这是一种混合策略：对于模型中比较"聪明"的关键部分，使用高一点的精度；对于不那么重要的部分，使用低一点的精度。与之对应的还有 ``S`` (Small，牺牲精度换更小、更快)和 ``L`` (Large,牺牲体积换更接近原始智力)

例如 ``q4_K_M`` 大约 43GB，则在我的双 :ref:`amd_radeon_instinct_mi50` 有64GB，则显存还剩 21GB，则剩下的21GB可以全部用来做 **KV Cache（上下文缓存）** ，这样能够分析大量文件，以及大量上下文不至于几个回合就"断片"。

``qwen3-coder:480b-a35b-q4_K_M``
======================================

- ``A3B``

  - A3B 代表 "Activated 3 Billion"（激活 30 亿参数）
  - 全量参数 (Total Parameters)：模型文件里总共有 480B（4800 亿）参数
  - 激活参数 (Active Parameters)：由于采用了 超稀疏混合专家（Ultra-Sparse MoE） 架构，当你输入一个词时，模型并不会动用全部 800 亿参数，而是仅激活其中的 3B（30 亿）核心参数进行计算

- 优势:

  - 速度极快：它的推理速度（TPS）接近于 3B 或 7B 的小模型，但在 4800 亿参数的底蕴下，智力却能达到 70B 以上 Dense 模型的水平
  - 显存需求：由于总参数是 480B，你仍然需要足够的显存来装下这 480B 的权重，但在计算时它非常省电且高效
