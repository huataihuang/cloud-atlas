# NOT USE -DGGML_CUDA=ON, ONLY support CPU
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=ON
