# NOT USE -DGGML_CUDA=ON, ONLY support CPU
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=ON
# -j 40 for system wich 48 cpu core
cmake --build llama.cpp/build --config Release --clean-first -j 40
#cp llama.cpp/build/bin/llama-* llama.cpp
